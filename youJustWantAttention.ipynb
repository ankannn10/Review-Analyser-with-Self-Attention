{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92eda94d-1c11-4f1c-bbe6-52f5253b4323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_short_reviews(file_path, label):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read().strip()\n",
    "    reviews = content.split()\n",
    "    labels = [label] * len(reviews)\n",
    "    lengths = ['short'] * len(reviews)\n",
    "    return reviews, labels, lengths\n",
    "\n",
    "# Load short positive and negative reviews\n",
    "short_positive_reviews, short_positive_labels, short_positive_lengths = load_short_reviews('positive-words.txt', 1)\n",
    "short_negative_reviews, short_negative_labels, short_negative_lengths = load_short_reviews('negative-words.txt', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a58ff28e-aac5-45e8-89ac-d86e98b5cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine short reviews and labels\n",
    "short_reviews = short_positive_reviews + short_negative_reviews\n",
    "short_labels = short_positive_labels + short_negative_labels\n",
    "short_lengths = short_positive_lengths + short_negative_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f32e94-abe9-4ec1-95dc-c0cce5f3ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(data_dir):\n",
    "    import os\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        dir_name = os.path.join(data_dir, label)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(dir_name, fname), encoding='utf-8') as f:\n",
    "                    texts.append(f.read())\n",
    "                    labels.append(1 if label == 'pos' else 0)\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = load_imdb_data('aclImdb/train')\n",
    "test_texts, test_labels = load_imdb_data('aclImdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8d0da7-74f8-45b4-934b-97cee5bbdd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine long reviews and labels\n",
    "long_reviews = train_texts + test_texts\n",
    "long_labels = train_labels + test_labels\n",
    "long_lengths = ['long'] * len(long_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ad67cd-c789-4642-805c-39fb169819e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame for long reviews\n",
    "long_df = pd.DataFrame({\n",
    "    'review': long_reviews,\n",
    "    'label': long_labels,\n",
    "    'length': long_lengths\n",
    "})\n",
    "\n",
    "# Create DataFrame for short reviews\n",
    "short_df = pd.DataFrame({\n",
    "    'review': short_reviews,\n",
    "    'label': short_labels,\n",
    "    'length': short_lengths\n",
    "})\n",
    "\n",
    "# Combine long and short reviews into one DataFrame\n",
    "all_reviews_df = pd.concat([long_df, short_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48625414-4361-4df0-beab-cfadf1566bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform stratified train-test split\n",
    "train_df, test_df = train_test_split(\n",
    "    all_reviews_df,\n",
    "    test_size=0.2,\n",
    "    stratify=all_reviews_df[['label', 'length']],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d63968f-a8d1-45ad-90c3-d1c5ca59e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      "label\n",
      "0    23826\n",
      "1    21605\n",
      "Name: count, dtype: int64\n",
      "length\n",
      "long     40000\n",
      "short     5431\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set distribution:\n",
      "label\n",
      "0    5957\n",
      "1    5401\n",
      "Name: count, dtype: int64\n",
      "length\n",
      "long     10000\n",
      "short     1358\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify the distribution in training set\n",
    "print(\"Training set distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(train_df['length'].value_counts())\n",
    "\n",
    "# Verify the distribution in test set\n",
    "print(\"\\nTest set distribution:\")\n",
    "print(test_df['label'].value_counts())\n",
    "print(test_df['length'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5a928c-7bf2-4c30-a4da-ba54436bdd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec96cee-c503-4e69-952d-26267323dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated TransformerEncoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.embedding = nn.Embedding(input_dim, model_dim)\n",
    "        self.pos_encoder = PositionalEncoding(model_dim, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(model_dim, 1)  # For binary classification\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.embedding(src) * math.sqrt(self.model_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=src_mask)\n",
    "        output = output.mean(dim=1)  # Global average pooling (dim=1 for batch_first=True)\n",
    "        output = self.fc_out(self.dropout(output))\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca03664-9f7f-470f-a844-486c34458a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db662d9-b959-4b69-ab64-24e4c7e94ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Hyperparameters\n",
    "INPUT_DIM = 10000  # Assuming this remains unchanged\n",
    "MODEL_DIM = 256    # Increased from 128 to 256\n",
    "NUM_HEADS = 8      # Increased from 4 to 8\n",
    "NUM_LAYERS = 4     # Increased from 2 to 4\n",
    "DROPOUT = 0.2      # Increased from 0.1 to 0.2\n",
    "BATCH_SIZE = 32    # Increased from 16 to 32\n",
    "MAX_SEQ_LEN = 256  # Increased from 128 to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df04b985-87e4-420d-8065-e01f3eeb43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the updated model\n",
    "model = TransformerEncoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    model_dim=MODEL_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5cb9322-0069-4313-8fbd-83d0eeba35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text.strip().split()\n",
    "\n",
    "# Build vocabulary from training data\n",
    "counter = Counter()\n",
    "for text in train_df['review']:\n",
    "    tokens = tokenize(text)\n",
    "    counter.update(tokens)\n",
    "\n",
    "# Keep most common words\n",
    "vocab_size = 10000\n",
    "most_common = counter.most_common(vocab_size - 2)  # Reserve 2 for PAD and UNK tokens\n",
    "word2idx = {word: idx + 2 for idx, (word, _) in enumerate(most_common)}\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1\n",
    "\n",
    "# Update INPUT_DIM\n",
    "INPUT_DIM = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb82c781-f336-4fb0-a989-72160a1d1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the encoding function to accommodate longer sequences\n",
    "def encode(text):\n",
    "    tokens = tokenize(text)\n",
    "    indices = [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n",
    "    if len(indices) > MAX_SEQ_LEN:\n",
    "        indices = indices[:MAX_SEQ_LEN]\n",
    "    else:\n",
    "        indices += [word2idx['<PAD>']] * (MAX_SEQ_LEN - len(indices))\n",
    "    return indices\n",
    "\n",
    "train_sequences = [encode(text) for text in train_df['review']]\n",
    "test_sequences = [encode(text) for text in test_df['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "501a9169-2a3f-463a-8453-e41c2c0ae310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_inputs = torch.LongTensor(train_sequences)\n",
    "train_labels_tensor = torch.FloatTensor(train_df['label'].tolist())\n",
    "test_inputs = torch.LongTensor(test_sequences)\n",
    "test_labels_tensor = torch.FloatTensor(test_df['label'].tolist())\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e22cba42-9ba6-4bd8-b35e-ddd8877d794f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (embedding): Embedding(10000, 256)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc_out): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the training loop with gradient clipping and learning rate scheduler\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37a1a9c7-c083-47aa-b2af-d988fb8789d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.5651\n",
      "Epoch 2/20, Loss: 0.4740\n",
      "Epoch 3/20, Loss: 0.4438\n",
      "Epoch 4/20, Loss: 0.4222\n",
      "Epoch 5/20, Loss: 0.4066\n",
      "Epoch 6/20, Loss: 0.3919\n",
      "Epoch 7/20, Loss: 0.3838\n",
      "Epoch 8/20, Loss: 0.3738\n",
      "Epoch 9/20, Loss: 0.3655\n",
      "Epoch 10/20, Loss: 0.3541\n",
      "Epoch 11/20, Loss: 0.3369\n",
      "Epoch 12/20, Loss: 0.3334\n",
      "Epoch 13/20, Loss: 0.3323\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     13\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     16\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adamw.py:227\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         state_steps,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m     adamw(\n\u001b[1;32m    228\u001b[0m         params_with_grad,\n\u001b[1;32m    229\u001b[0m         grads,\n\u001b[1;32m    230\u001b[0m         exp_avgs,\n\u001b[1;32m    231\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    232\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    233\u001b[0m         state_steps,\n\u001b[1;32m    234\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    235\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    236\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    237\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    238\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    239\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    240\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    241\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    242\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    243\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    244\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    245\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    246\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    247\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adamw.py:767\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 767\u001b[0m func(\n\u001b[1;32m    768\u001b[0m     params,\n\u001b[1;32m    769\u001b[0m     grads,\n\u001b[1;32m    770\u001b[0m     exp_avgs,\n\u001b[1;32m    771\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    772\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    773\u001b[0m     state_steps,\n\u001b[1;32m    774\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    775\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    776\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    777\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    778\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    779\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    780\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    781\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    782\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    783\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    784\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    785\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    786\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adamw.py:377\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    374\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m param\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    380\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20  # Increased number of epochs to accommodate deeper model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    scheduler.step()  # Update learning rate\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation remains similar, with detailed metrics as previously outlined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a10f4743-632a-4449-9a61-c3eebaa7e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IndexedTensorDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.labels[index], index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "test_dataset = IndexedTensorDataset(test_inputs, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1903f381-4bb9-4309-a67b-dc85c2e4254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation loop\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_lengths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, indices in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Get review lengths from test_df using indices\n",
    "        batch_lengths = test_df.iloc[indices.numpy()]['length'].tolist()\n",
    "        all_lengths.extend(batch_lengths)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_lengths = np.array(all_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17762aa8-064e-475f-a8ea-1ed5d974bc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8366\n",
      "\n",
      "Overall Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.86      0.85      5957\n",
      "    Positive       0.84      0.81      0.82      5401\n",
      "\n",
      "    accuracy                           0.84     11358\n",
      "   macro avg       0.84      0.84      0.84     11358\n",
      "weighted avg       0.84      0.84      0.84     11358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall metrices\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "target_names = ['Negative', 'Positive']\n",
    "print(\"\\nOverall Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7677d594-845e-405f-8c04-4691d7cbebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for Long Reviews:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.85      0.86      5000\n",
      "    Positive       0.85      0.86      0.86      5000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "Confusion Matrix for Long Reviews:\n",
      "[[4256  744]\n",
      " [ 686 4314]]\n",
      "\n",
      "Classification Report for Short Reviews:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.71      0.93      0.81       957\n",
      "    Positive       0.38      0.10      0.16       401\n",
      "\n",
      "    accuracy                           0.69      1358\n",
      "   macro avg       0.55      0.52      0.48      1358\n",
      "weighted avg       0.62      0.69      0.62      1358\n",
      "\n",
      "Confusion Matrix for Short Reviews:\n",
      "[[891  66]\n",
      " [360  41]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrices broken down by review length\n",
    "lengths = np.unique(all_lengths)\n",
    "\n",
    "for length in lengths:\n",
    "    indices = np.where(all_lengths == length)\n",
    "    length_labels = all_labels[indices]\n",
    "    length_preds = all_preds[indices]\n",
    "    \n",
    "    print(f\"\\nClassification Report for {length.capitalize()} Reviews:\")\n",
    "    print(classification_report(length_labels, length_preds, target_names=target_names))\n",
    "    \n",
    "    cm_length = confusion_matrix(length_labels, length_preds)\n",
    "    print(f\"Confusion Matrix for {length.capitalize()} Reviews:\")\n",
    "    print(cm_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "867d6bb0-e219-4316-98e4-1240479c1671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Negative Class:\n",
      "Precision: 1.0000\n",
      "Recall:    0.8640\n",
      "F1-Score:  0.9271\n",
      "\n",
      "Metrics for Positive Class:\n",
      "Precision: 1.0000\n",
      "Recall:    0.8063\n",
      "F1-Score:  0.8928\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrices broken down by class\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "for class_label, class_name in enumerate(target_names):\n",
    "    indices = np.where(all_labels == class_label)\n",
    "    class_labels = all_labels[indices]\n",
    "    class_preds = all_preds[indices]\n",
    "    \n",
    "    precision = precision_score(class_labels, class_preds, pos_label=class_label)\n",
    "    recall = recall_score(class_labels, class_preds, pos_label=class_label)\n",
    "    f1 = f1_score(class_labels, class_preds, pos_label=class_label)\n",
    "    \n",
    "    print(f\"\\nMetrics for {class_name} Class:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba1b67d6-6c11-450c-8401-b544c9b313a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pickle\\n\\n# Save the trained model\\ntorch.save(model.state_dict(), 'sentiment_model.pt')\\n\\n# Save the word2idx mapping\\nwith open('word2idx.pkl', 'wb') as f:\\n    pickle.dump(word2idx, f)\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'sentiment_model.pt')\n",
    "\n",
    "# Save the word2idx mapping\n",
    "with open('word2idx.pkl', 'wb') as f:\n",
    "    pickle.dump(word2idx, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe9ad774-d3ad-4f96-8468-bac77b21baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state after 12 epochs\n",
    "torch.save(model.state_dict(), 'model_after_12_epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9fae508-a119-45d4-8c8e-566c0a22b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qz/3374k0z123qd44rjjjhkmxnr0000gn/T/ipykernel_42471/1385275984.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_after_12_epochs.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model state\n",
    "model.load_state_dict(torch.load('model_after_12_epochs.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdcc326e-1add-40a3-b145-5c82d670aa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 0.3297\n",
      "Epoch 16/20, Loss: 0.3283\n",
      "Epoch 17/20, Loss: 0.3262\n",
      "Epoch 18/20, Loss: 0.3243\n",
      "Epoch 19/20, Loss: 0.3245\n",
      "Epoch 20/20, Loss: 0.3228\n",
      "Epoch 21/20, Loss: 0.3190\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 14\n",
    "num_epochs = 20  # Increased number of epochs to accommodate deeper model\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    #scheduler.step()  # Update learning rate\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation remains similar, with detailed metrics as previously outlined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "792e2d28-a419-492a-bf1e-caa4ad0d87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IndexedTensorDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.labels[index], index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "test_dataset = IndexedTensorDataset(test_inputs, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49820d82-d254-446d-9bdb-e229c7ccb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation loop\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_lengths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, indices in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Get review lengths from test_df using indices\n",
    "        batch_lengths = test_df.iloc[indices.numpy()]['length'].tolist()\n",
    "        all_lengths.extend(batch_lengths)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_lengths = np.array(all_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "833cf41e-b099-48d5-abed-0f1bbbdaddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8376\n",
      "\n",
      "Overall Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.82      0.88      0.85      5957\n",
      "    Positive       0.86      0.79      0.82      5401\n",
      "\n",
      "    accuracy                           0.84     11358\n",
      "   macro avg       0.84      0.84      0.84     11358\n",
      "weighted avg       0.84      0.84      0.84     11358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall metrices\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "target_names = ['Negative', 'Positive']\n",
    "print(\"\\nOverall Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd426da0-9960-4013-8455-126f0fea63c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for Long Reviews:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.88      0.86      5000\n",
      "    Positive       0.87      0.84      0.86      5000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "Confusion Matrix for Long Reviews:\n",
      "[[4401  599]\n",
      " [ 815 4185]]\n",
      "\n",
      "Classification Report for Short Reviews:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.72      0.90      0.80       957\n",
      "    Positive       0.41      0.16      0.23       401\n",
      "\n",
      "    accuracy                           0.68      1358\n",
      "   macro avg       0.56      0.53      0.51      1358\n",
      "weighted avg       0.63      0.68      0.63      1358\n",
      "\n",
      "Confusion Matrix for Short Reviews:\n",
      "[[865  92]\n",
      " [338  63]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrices broken down by review length\n",
    "lengths = np.unique(all_lengths)\n",
    "\n",
    "for length in lengths:\n",
    "    indices = np.where(all_lengths == length)\n",
    "    length_labels = all_labels[indices]\n",
    "    length_preds = all_preds[indices]\n",
    "    \n",
    "    print(f\"\\nClassification Report for {length.capitalize()} Reviews:\")\n",
    "    print(classification_report(length_labels, length_preds, target_names=target_names))\n",
    "    \n",
    "    cm_length = confusion_matrix(length_labels, length_preds)\n",
    "    print(f\"Confusion Matrix for {length.capitalize()} Reviews:\")\n",
    "    print(cm_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f6805f-0330-4bab-acc4-47aecf04c82d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate metrices broken down by class\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score, f1_score\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_label, class_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(target_names):\n\u001b[1;32m      5\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(all_labels \u001b[38;5;241m==\u001b[39m class_label)\n\u001b[1;32m      6\u001b[0m     class_labels \u001b[38;5;241m=\u001b[39m all_labels[indices]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_names' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate metrices broken down by class\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "for class_label, class_name in enumerate(target_names):\n",
    "    indices = np.where(all_labels == class_label)\n",
    "    class_labels = all_labels[indices]\n",
    "    class_preds = all_preds[indices]\n",
    "    \n",
    "    precision = precision_score(class_labels, class_preds, pos_label=class_label)\n",
    "    recall = recall_score(class_labels, class_preds, pos_label=class_label)\n",
    "    f1 = f1_score(class_labels, class_preds, pos_label=class_label)\n",
    "    \n",
    "    print(f\"\\nMetrics for {class_name} Class:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d930803-48fe-4f2f-869d-69ee43e2feca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
