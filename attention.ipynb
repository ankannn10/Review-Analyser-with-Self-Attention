{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6064c05-a370-4aa5-9f8d-7c9322b2488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "077f7ffa-b29e-4020-8685-0a41d4c359b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download dataset\n",
    "'''!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xzf aclImdb_v1.tar.gz'''\n",
    "\n",
    "'''import wget\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "wget.download(url, \"aclImdb_v1.tar.gz\")'''\n",
    "\n",
    "\n",
    "# Load data\n",
    "def load_imdb_data(data_dir):\n",
    "    import os\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        dir_name = os.path.join(data_dir, label)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(dir_name, fname), encoding='utf-8') as f:\n",
    "                    texts.append(f.read())\n",
    "                    labels.append(1 if label == 'pos' else 0)\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = load_imdb_data('attention/train')\n",
    "test_texts, test_labels = load_imdb_data('attention/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9227b19-b1c9-444c-8876-f8150edc6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.embedding = nn.Embedding(input_dim, model_dim)\n",
    "        self.pos_encoder = PositionalEncoding(model_dim, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first = True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(model_dim, 1)  # For binary classification\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.embedding(src) * math.sqrt(self.model_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=src_mask)\n",
    "        output = output.mean(dim=0)  # Global average pooling\n",
    "        output = self.fc_out(self.dropout(output))\n",
    "        return output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c8e10b8-82b2-4ccd-90bc-8b5758eceb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c441637c-1d73-48bd-a399-31b09eb48e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text.strip().split()\n",
    "\n",
    "# Build vocabulary\n",
    "counter = Counter()\n",
    "for text in train_texts:\n",
    "    tokens = tokenize(text)\n",
    "    counter.update(tokens)\n",
    "\n",
    "# Keep most common words\n",
    "vocab_size = 10000\n",
    "most_common = counter.most_common(vocab_size - 2)  # Reserve 2 for PAD and UNK tokens\n",
    "word2idx = {word: idx + 2 for idx, (word, _) in enumerate(most_common)}\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1\n",
    "\n",
    "# Create inverse mapping\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Update INPUT_DIM\n",
    "INPUT_DIM = vocab_size # To be defined after building the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "942f714f-3926-4b05-b3c7-1f8c63226cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 16  # Adjust based on available memory\n",
    "MAX_SEQ_LEN = 128  # Truncate or pad sequences to this length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b12dc7e-19f2-4d9a-a7ab-441917c1d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    model_dim=MODEL_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ed9f737-8221-45c7-8ccf-05a46472d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    tokens = tokenize(text)\n",
    "    indices = [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n",
    "    if len(indices) > MAX_SEQ_LEN:\n",
    "        indices = indices[:MAX_SEQ_LEN]\n",
    "    else:\n",
    "        indices += [word2idx['<PAD>']] * (MAX_SEQ_LEN - len(indices))\n",
    "    return indices\n",
    "\n",
    "train_sequences = [encode(text) for text in train_texts]\n",
    "test_sequences = [encode(text) for text in test_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f673a0ce-533c-4215-903c-d56326112b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_inputs = torch.LongTensor(train_sequences)\n",
    "train_labels = torch.FloatTensor(train_labels)\n",
    "test_inputs = torch.LongTensor(test_sequences)\n",
    "test_labels = torch.FloatTensor(test_labels)\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36a3b7d5-20cf-4602-a711-3f2e10f1d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbb2b804-bf45-4a00-a279-3ba2d99fe14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (embedding): Embedding(10000, 128)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc_out): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a70da7b-71f6-457f-a7d0-1f6f29924d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.6415\n",
      "Epoch 2/40, Loss: 0.5528\n",
      "Epoch 3/40, Loss: 0.5117\n",
      "Epoch 4/40, Loss: 0.4854\n",
      "Epoch 5/40, Loss: 0.4623\n",
      "Epoch 6/40, Loss: 0.4415\n",
      "Epoch 7/40, Loss: 0.4234\n",
      "Epoch 8/40, Loss: 0.4091\n",
      "Epoch 9/40, Loss: 0.3959\n",
      "Epoch 10/40, Loss: 0.3839\n",
      "Epoch 11/40, Loss: 0.3738\n",
      "Epoch 12/40, Loss: 0.3662\n",
      "Epoch 13/40, Loss: 0.3558\n",
      "Epoch 14/40, Loss: 0.3488\n",
      "Epoch 15/40, Loss: 0.3452\n",
      "Epoch 16/40, Loss: 0.3363\n",
      "Epoch 17/40, Loss: 0.3272\n",
      "Epoch 18/40, Loss: 0.3246\n",
      "Epoch 19/40, Loss: 0.3177\n",
      "Epoch 20/40, Loss: 0.3113\n",
      "Epoch 21/40, Loss: 0.3071\n",
      "Epoch 22/40, Loss: 0.3017\n",
      "Epoch 23/40, Loss: 0.2978\n",
      "Epoch 24/40, Loss: 0.2937\n",
      "Epoch 25/40, Loss: 0.2895\n",
      "Epoch 26/40, Loss: 0.2839\n",
      "Epoch 27/40, Loss: 0.2825\n",
      "Epoch 28/40, Loss: 0.2773\n",
      "Epoch 29/40, Loss: 0.2760\n",
      "Epoch 30/40, Loss: 0.2703\n",
      "Epoch 31/40, Loss: 0.2684\n",
      "Epoch 32/40, Loss: 0.2666\n",
      "Epoch 33/40, Loss: 0.2621\n",
      "Epoch 34/40, Loss: 0.2602\n",
      "Epoch 35/40, Loss: 0.2569\n",
      "Epoch 36/40, Loss: 0.2553\n",
      "Epoch 37/40, Loss: 0.2519\n",
      "Epoch 38/40, Loss: 0.2488\n",
      "Epoch 39/40, Loss: 0.2474\n",
      "Epoch 40/40, Loss: 0.2440\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.transpose(0, 1).to(device)  # Transformer expects seq_len x batch_size\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd1ca637-0ff2-49f0-bc29-08fff1475061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8321\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.transpose(0, 1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4df8fc2b-6c88-470e-82a2-aa73f283c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'attention_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
